{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9ebzfXKeMdh",
        "outputId": "903e29a2-9c44-4ceb-b8c0-a6de9b37eb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4432/4432 [==============================] - 9s 2ms/step - loss: 0.9823 - accuracy: 0.5137\n",
            "Epoch 2/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.9155 - accuracy: 0.5654\n",
            "Epoch 3/20\n",
            "4432/4432 [==============================] - 7s 2ms/step - loss: 0.8837 - accuracy: 0.5864\n",
            "Epoch 4/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.8642 - accuracy: 0.5992\n",
            "Epoch 5/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.8514 - accuracy: 0.6052\n",
            "Epoch 6/20\n",
            "4432/4432 [==============================] - 7s 2ms/step - loss: 0.8422 - accuracy: 0.6101\n",
            "Epoch 7/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.8336 - accuracy: 0.6142\n",
            "Epoch 8/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.8274 - accuracy: 0.6180\n",
            "Epoch 9/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.8219 - accuracy: 0.6216\n",
            "Epoch 10/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.8165 - accuracy: 0.6245\n",
            "Epoch 11/20\n",
            "4432/4432 [==============================] - 7s 2ms/step - loss: 0.8120 - accuracy: 0.6268\n",
            "Epoch 12/20\n",
            "4432/4432 [==============================] - 9s 2ms/step - loss: 0.8087 - accuracy: 0.6277\n",
            "Epoch 13/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.8053 - accuracy: 0.6307\n",
            "Epoch 14/20\n",
            "4432/4432 [==============================] - 7s 2ms/step - loss: 0.8019 - accuracy: 0.6316\n",
            "Epoch 15/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.7991 - accuracy: 0.6343\n",
            "Epoch 16/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.7966 - accuracy: 0.6360\n",
            "Epoch 17/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.7937 - accuracy: 0.6366\n",
            "Epoch 18/20\n",
            "4432/4432 [==============================] - 9s 2ms/step - loss: 0.7919 - accuracy: 0.6362\n",
            "Epoch 19/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.7898 - accuracy: 0.6368\n",
            "Epoch 20/20\n",
            "4432/4432 [==============================] - 8s 2ms/step - loss: 0.7878 - accuracy: 0.6398\n",
            "1478/1478 - 2s - loss: 0.8006 - accuracy: 0.6343 - 2s/epoch - 1ms/step\n",
            "Loss: 0.8005675673484802, Accuracy: 0.6343021392822266\n"
          ]
        }
      ],
      "source": [
        "#Used categorical_crossentropy as loss function since it's more suitable for multi-class classification.\n",
        "#Used softmax activation in the output layer for multi-class classification.\n",
        "\n",
        "\n",
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#  Import and read the csv file.\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/DanielHughes1580/Project_4_Group_5/main/Data_Cleaned/dummied_data.csv\", index_col=0)\n",
        "\n",
        "# Convert categorical labels to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['status_encoded'] = label_encoder.fit_transform(df['Status'])\n",
        "\n",
        "# Convert numerical labels to one-hot encoded format\n",
        "encoded_status = to_categorical(df['status_encoded'])\n",
        "\n",
        "\n",
        "# Perform one-hot encoding on the 'Status' column\n",
        "#df = pd.get_dummies(df, columns=['Status'])\n",
        "#df_encoded.head()\n",
        "\n",
        "# Split our preprocessed data into our features and target arrays\n",
        "X = df.drop(columns=['Status', 'status_encoded'])  # Features\n",
        "y = encoded_status  # Target variable\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "input_features = X_train_scaled.shape[1]\n",
        "\n",
        "nn = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=50, input_dim=input_features, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=50, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=y.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "nn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=20, verbose=1)\n",
        "\n",
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[100:101]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVC7r_vWjTeG",
        "outputId": "462c69d1-b6f8-4e43-89ae-453384372183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.predict(X_test_scaled[100:101])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TCQYtH-jdoa",
        "outputId": "7e70d62e-519c-4df2-fd42-f132cbe1ebbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.37937946e-08, 2.43946910e-01, 5.97092927e-01, 1.58960178e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}